{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[v2] Models, train, test",
      "provenance": [],
      "collapsed_sections": [
        "WzBMXSCGVmyW",
        "af52Pzmle5Gv",
        "R4END9Ase9nt",
        "OzwO6Bh3EOBi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IciaCarroBarallobre/AdventOfCode/blob/main/notebooks/%5Bv2%5D_Models%2C_train%2C_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "\n",
        "# Models, train e test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHJwIXnmFVWL"
      },
      "source": [
        " ## Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d-sff0BfYff",
        "outputId": "1c896d4c-85ff-451e-d75c-344c221475a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rs2o9X9PsH7"
      },
      "source": [
        "## Hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViW719zKP5oP"
      },
      "source": [
        "#################    DATASET    ##############################\n",
        "ROOT = '/content/drive/My Drive/TFG/'\n",
        "ROOT_DATA = ROOT + 'data/'\n",
        "ROOT_DATASETS = ROOT_DATA +'datasets/'\n",
        "DATASET = 'subrolldataset'\n",
        "OCT_DEVICE = 'SPECTRALIS'\n",
        "CLASS = \"SRD\" # DME - DRT SRD CME\n",
        "RANDOM = \"proporcional_by_id\" # \"total\"|\"by_id\"|\"proporcional_by_id\"| \"proporcional_by_id_giving_a_test\"\n",
        "LABELS_CSV = \"/info_labels2.csv\" # \"/info.csv\"\n",
        "################     THRESHOLD    ##################################\n",
        "# Only apply at negative images\n",
        "DOUBT, MAX_DOUBT = False, None\n",
        "\n",
        "if DOUBT == True:\n",
        "  if CLASS == \"DRT\": # 0.30 - 75, 90 -0.4214\n",
        "    MAX_DOUBT = 0.4214\n",
        "  elif CLASS == \"CME\": # 0.25 - 75, 90- 0.3874\n",
        "    MAX_DOUBT = 0.3874\n",
        "  elif CLASS == \"SRD\": # 0.5 - 75, 90 - 0.533\n",
        "    MAX_DOUBT =  0.533\n",
        "\n",
        "################     MODEL    ##################################\n",
        "MODEL_STR = 'Densenet161' \n",
        "OPTIM, LR  = 'Adam', 0.001\n",
        "DEBUG = 1\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "BATCH = 8 # More than 16, cuda out of memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2O6HaIwRwj"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnLehldV59kF"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random \n",
        "\n",
        "import torchvision\n",
        "from torchvision import  models, transforms\n",
        "\n",
        "import torch \n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "\n",
        "from PIL import Image \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7BJTKNFFIyM"
      },
      "source": [
        "## Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "\n",
        "### <strong> OCTDataset:  </strong> Subclase de [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLh7ageMViB-"
      },
      "source": [
        "**Dataset** é unha clase abstracta que representa a un conxunto de  **samples**. Un **sample** é unha **dupla** (input,label). \n",
        "\n",
        "As subclases de dataset poden sobrescribir as seguintes funcións:\n",
        "\n",
        "*   *__getitem __(self,idx)* [OBL]: Obten un sample para unha  determinada clave  *idx*. \n",
        "*   *__len __ ()*[OPT]: Devolve o tamaño do conxunto de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "source": [
        "class OCTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, max_doubt= None, device = None,transform=None, dataset_name = LABELS_CSV):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "        self.max_doubt = max_doubt\n",
        "        self.annotations = pd.read_csv(root+dataset_name) \n",
        "\n",
        "        if self.device is not None:\n",
        "          self.annotations = (self.annotations [self.annotations [\"device\"] == device]).reset_index(drop=True)\n",
        "\n",
        "        if self.max_doubt is not None:  \n",
        "          self.annotations.drop(self.annotations[(self.annotations[CLASS] == 0) & (self.annotations[\"doubt_percentage\"] > self.max_doubt)].index, inplace = True)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        image = Image.open(self.annotations[\"root\"][idx] ).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        label = np.array([self.annotations[CLASS][idx]])\n",
        "        name =  self.annotations[\"name\"][idx] \n",
        "        \n",
        "        return image, label, name #sample\n",
        "\n",
        "    def info(self):\n",
        "      return self.annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ieqj5uJPgm"
      },
      "source": [
        "### **DataLoader**: Dataset to Iterable\n",
        "\n",
        "Para poder iterar sobre un dataset necesitamos un [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lh-tNc5gf2w"
      },
      "source": [
        "#### Split by idxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3oGm4O-WNxd"
      },
      "source": [
        "def random_split_by_idxs(dataset, split_list):\n",
        "  \n",
        "  p_train, p_val = split_list[0], split_list[1]\n",
        "  \n",
        "  df = dataset.info()\n",
        "  df_ids = df.groupby([\"id\"]).count()[\"root\"].to_frame()\n",
        "  \n",
        "  rest, test_ids= train_test_split(list(df_ids.index), test_size = 1 - p_train - p_val )\n",
        "  train_ids, val_ids = train_test_split(rest, test_size = p_val)\n",
        "\n",
        "  train_idx = df[df.id.isin(train_ids)]\n",
        "  val_idx   = df[df.id.isin(val_ids)]\n",
        "  test_idx  = df[df.id.isin(test_ids)]\n",
        "  \n",
        "  train = Subset(dataset, indices=list(train_idx.index))\n",
        "  val = Subset(dataset, indices=list(val_idx.index))\n",
        "  test = Subset(dataset, indices=list(test_idx.index))\n",
        "  \n",
        "  return train, val, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnFLoqaR_jbT"
      },
      "source": [
        "DEBUG  = 0 #test\n",
        "\n",
        "def proporcional_random_split_by_idxs(dataset, split_list):\n",
        "  \n",
        "  p_train, p_val = split_list[0], split_list[1]\n",
        "  \n",
        "  df = dataset.info()\n",
        "  df_ids = df.groupby([\"id\"]).sum()[CLASS].to_frame()\n",
        "  positive = df_ids[df_ids[CLASS] > 0]\n",
        "  negative = df_ids[df_ids[CLASS] == 0]\n",
        "\n",
        "  #Positive proportion\n",
        "  rest_of_positive, positive_test_ids= train_test_split(list(positive.index), test_size = 1 - p_train - p_val )\n",
        "  positive_train_ids, positive_val_ids = train_test_split(rest_of_positive, test_size = p_val)\n",
        "  \n",
        "  #Negative proportion\n",
        "  rest_of_negative, negative_test_ids = train_test_split(list(negative.index), test_size = 1 - p_train - p_val )\n",
        "  negative_train_ids, negative_val_ids = train_test_split(rest_of_negative, test_size = p_val)\n",
        "\n",
        "  # Join \n",
        "  train_ids =  negative_train_ids + positive_train_ids\n",
        "  test_ids =  negative_test_ids + positive_test_ids\n",
        "  val_ids =  negative_val_ids + positive_val_ids\n",
        "\n",
        "  #Shuffle\n",
        "  random.shuffle(train_ids)\n",
        "  random.shuffle(test_ids)\n",
        "  random.shuffle(val_ids)\n",
        "\n",
        "  train_idx = df[df.id.isin(train_ids)]\n",
        "  val_idx   = df[df.id.isin(val_ids)]\n",
        "  test_idx  = df[df.id.isin(test_ids)]\n",
        "  \n",
        "  train = Subset(dataset, indices=list(train_idx.index))\n",
        "  val = Subset(dataset, indices=list(val_idx.index))\n",
        "  test = Subset(dataset, indices=list(test_idx.index))\n",
        "  \n",
        "  return train, val, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4840k4ytZcP"
      },
      "source": [
        "def proporcional_random_split_by_idxs_giving_a_test(dataset, val):\n",
        "  \n",
        "  df = dataset.info()\n",
        "  df_ids = df.groupby([\"id\"]).sum()[CLASS].to_frame()\n",
        "  positive = df_ids[df_ids[CLASS] > 0]\n",
        "  negative = df_ids[df_ids[CLASS] == 0]\n",
        "\n",
        "  test_idx = df[df[\"test_\"+CLASS] == True].index\n",
        "  df = df[df[\"test\"==False]]\n",
        "\n",
        "  #Positive proportion\n",
        "  positive_val_ids, positive_train_ids= train_test_split(list(positive.index), test_size = 1 - val )\n",
        "  negative_val_ids, negative_train_ids= train_test_split(list(negative.index), test_size = 1 - p_val )\n",
        "  train_ids =  negative_train_ids + positive_train_ids\n",
        "  val_ids =  negative_val_ids + positive_val_ids\n",
        "\n",
        "  #Shuffle\n",
        "  random.shuffle(train_ids)\n",
        "  random.shuffle(test_ids)\n",
        "  random.shuffle(val_ids)\n",
        "\n",
        "  train_idx = df[df.id.isin(train_ids)]\n",
        "  val_idx   = df[df.id.isin(val_ids)]\n",
        "  test_idx  = df[df.id.isin(test_ids)]\n",
        "  \n",
        "  train = Subset(dataset, indices=list(train_idx.index))\n",
        "  val = Subset(dataset, indices=list(val_idx.index))\n",
        "  test = Subset(dataset, indices=list(test_idx.index))\n",
        "  \n",
        "  return train, val, test  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e33injHmgkpb"
      },
      "source": [
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZzq9sNdK0ol"
      },
      "source": [
        "def data_loader_fun(root_dataset_dir):\n",
        "\n",
        "    image_transforms =  transforms.Compose([\n",
        "        transforms.Resize((224,224)), \n",
        "        transforms.RandomHorizontalFlip(p = 0.5), # Poden aparecer patrones en calquer dir\n",
        "        transforms.ToTensor() #Os anteriores traballan con PIL, non con tensores\n",
        "        ])\n",
        "\n",
        "    dataset = OCTDataset(root_dataset_dir, device=OCT_DEVICE, max_doubt = MAX_DOUBT, transform=image_transforms)\n",
        "    dataset.info().count()\n",
        "    \n",
        "    #### SPLIT DATASET\n",
        "    percentage_train = 0.6\n",
        "    percentage_val = 0.2\n",
        "\n",
        "    if RANDOM == \"total\":\n",
        "      total = len(dataset)\n",
        "      n_train, n_val = math.floor(total * percentage_train), math.floor(total *percentage_val)\n",
        "      n_test = total - n_train - n_val\n",
        "      train_set, val_set, test_set = random_split(dataset, [n_train,n_val,n_test])\n",
        "      \n",
        "    elif RANDOM == \"by_id\":\n",
        "      train_set, val_set, test_set = random_split_by_idxs(dataset, [percentage_train,percentage_val])\n",
        "      n_train, n_val, n_test = len(train_set), len(val_set), len(test_set)\n",
        "      total = n_train + n_val + n_test\n",
        "\n",
        "    elif RANDOM == \"proporcional_by_id\":\n",
        "      result = proporcional_random_split_by_idxs(dataset, [percentage_train,percentage_val])\n",
        "      train_set, val_set, test_set = result \n",
        "      n_train, n_val, n_test = len(train_set), len(val_set), len(test_set)\n",
        "      total = n_train + n_val + n_test\n",
        "    elif RANDOM == \"proporcional_by_id_giving_a_test\":\n",
        "      result = proporcional_random_split_by_idxs_giving_a_test(dataset, percentage_val)   \n",
        "      train_set, val_set, test_set = result \n",
        "      n_train, n_val, n_test = len(train_set), len(val_set), len(test_set)\n",
        "      total = n_train + n_val + n_test\n",
        "\n",
        "    print(\"Total images in dataset: \",total,\"\\n Train:\",n_train,\" Val:\",n_val,\" Test:\", n_test )\n",
        "\n",
        "    data_generator = {'train': train_set, 'val': val_set, 'test': test_set}\n",
        "                             \n",
        "    return {k: DataLoader(data_generator[k], batch_size=BATCH, shuffle= True) \n",
        "    for k in ['train', 'val','test']}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HBhIdOSDXXY",
        "outputId": "f091d80f-0f27-41b2-abc1-70ff71af9150"
      },
      "source": [
        "data_loader_fun(ROOT_DATASETS+DATASET)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total images in dataset:  2623 \n",
            " Train: 1667  Val: 426  Test: 530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <torch.utils.data.dataloader.DataLoader at 0x7fe2abb61fd0>,\n",
              " 'train': <torch.utils.data.dataloader.DataLoader at 0x7fe2abb92190>,\n",
              " 'val': <torch.utils.data.dataloader.DataLoader at 0x7fe2abb920d0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzBMXSCGVmyW"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNmNLHTG8YR3"
      },
      "source": [
        "### EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JpN0EbZVvaD"
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience, delta=0):\n",
        "            \n",
        "        self.patience = patience # Canto tempo esperar despois do mellor loss en val.\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta # Cambio minimo para contabilizar  \n",
        "        self.model = model\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score =- val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.model = model\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if DEBUG > 1:\n",
        "              print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.model = model\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, path):    # Saves model when validation loss decrease.\n",
        "        torch.save(model.state_dict(),path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RzCmX7mEngJ"
      },
      "source": [
        "### Fun. activacion \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w92JZ9wE7Cd"
      },
      "source": [
        "\n",
        "[Softmax](https://pytorch.org/docs/master/_modules/torch/nn/modules/activation.html#Softmax): Emplease para \"comprimir\" un vector K-dimensional $z$, de valores reales arbitrarios nun vector K-dimensional, $σ(z)$, de valores reales no rango $[0, 1]$. \n",
        "\n",
        "$f(s)_{i} = \\frac{e^{s_{i}}}{\\sum_{j}^{C} e^{s_{j}}}$\n",
        "\n",
        "Sendo $s_{j}$ a puntuación inferida pola red para cada clase en C. Os resultados de softmax dependenden do resto de clases.\n",
        "\n",
        " ![Softmax](https://drive.google.com/uc?id=18p8tB8pn_tl7SgB2CRTWXsx1KQ3MX0M8)\n",
        "\n",
        " [**Sigmoid**:](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) $\\frac{1}{1+e^{(-x)}}$\n",
        "\n",
        " ![Sigmoid](https://drive.google.com/uc?id=1GnIcpt1T6I17Z8OZ6D7JNy5UBwRsOdvl)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-sl6E5Ccw0"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HffuCTmXE_dX"
      },
      "source": [
        "\n",
        "**CrossEntropy**\n",
        "\n",
        "$CE = -\\sum_{i}^{C}t_{i} log (s_{i})$\n",
        "\n",
        "Onde $t_{i }$ son as labels e $s_{i}$ son as puntaciones para cada clase i en C. \n",
        "\n",
        "Soese usar as funciones (Sigmoid / Softmax) antes de la CE.\n",
        "\n",
        "*   Binary cross entropy [(nn.BCELoss)](https://pytorch.org/docs/master/generated/torch.nn.BCELoss.html)\n",
        "*   Cross entropy en Torch [(nn.CrossEntropyLoss)](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) usa:\n",
        "  *   [ nn.LogSoftmax()](https://pytorch.org/docs/master/generated/torch.nn.LogSoftmax.html)\n",
        "\n",
        "  *   [ nn.NLLLoss()](https://pytorch.org/docs/master/generated/torch.nn.NLLLoss.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEdf4-vHVvFr"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvBxKc2nGYiY"
      },
      "source": [
        "########### Load Bar\n",
        "def print_bar(now, maximum, max_length = 20):\n",
        "    done = now/maximum\n",
        "    print(\"\\r\"+round(done*max_length)*\"█\"+round(max_length*(1-done))*\"░\"+\" \"+str(round(done*100,2))+\"% \", end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzkrvizKVp6O"
      },
      "source": [
        "def train_model(device, model, data_loaders, criterion, optimizer, epochs = 50):\n",
        "    \n",
        "    early_stopping = EarlyStopping(patience=PATIENCE)\n",
        "    train_loss, val_loss, acc_train, acc_val = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print_bar(epoch+1, epochs)\n",
        "        for phase in ['train','val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss, correct = 0.0, 0\n",
        "            \n",
        "            for inputs, labels, _ in data_loaders[phase]: \n",
        "                        \n",
        "                inputs = inputs.to(device)          \n",
        "                optimizer.zero_grad() \n",
        "\n",
        "                #We don't want calculte gradients when we're val \n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    outputs = (model(inputs)).to(device) \n",
        "                    labels = torch.flatten(labels).type(torch.LongTensor).to(device) \n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1) \n",
        "                    if phase == 'train':\n",
        "                        loss.backward() \n",
        "                        optimizer.step() \n",
        "\n",
        "                running_loss += loss.item()  * inputs.size(0)\n",
        "                correct += torch.sum(preds == labels).item()\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
        "            epoch_acc = correct / len(data_loaders[phase].dataset)\n",
        "            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == \"train\":\n",
        "                train_loss.append(epoch_loss)\n",
        "                acc_train.append(epoch_acc)\n",
        "                \n",
        "            elif phase == \"val\":\n",
        "                val_loss.append(epoch_loss)\n",
        "                acc_val.append(epoch_acc)\n",
        "                early_stopping(epoch_loss, model)\n",
        "                if early_stopping.early_stop:\n",
        "                    return train_loss, val_loss, epochs, acc_train,acc_val, early_stopping\n",
        "                                    \n",
        "    return train_loss, val_loss, epochs, acc_train,acc_val, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af52Pzmle5Gv"
      },
      "source": [
        "## Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmAJ3AQ2e88q"
      },
      "source": [
        "def test(device, model, optimizer, data_loaders):\n",
        "  \n",
        "    model.eval()\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    y_true, y_pred, score = [], [],[]            \n",
        "    list_fp,list_fn = [], []\n",
        "\n",
        "    for inputs, labels, names in data_loaders[\"test\"]:\n",
        "            \n",
        "        inputs = inputs.to(device)\n",
        "        labels = torch.flatten(labels).type(torch.LongTensor).to(device)\n",
        "        \n",
        "        outputs = (model(inputs)).to(device) \n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        score.extend(outputs.tolist())\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            if (preds[i] == 1) and (labels[i] == 0):\n",
        "                list_fp.append(names[i])\n",
        "            elif (preds[i] == 0) and (labels[i] == 1):\n",
        "                list_fn.append(names[i])\n",
        "              \n",
        "                \n",
        "    score =  softmax(torch.tensor(score).to(device))\n",
        "    score,_ = torch.max(score, 1)\n",
        "    \n",
        "    return y_pred, y_true, score.tolist(), list_fp, list_fn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4END9Ase9nt"
      },
      "source": [
        "### Metricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVAMyYvFkm9"
      },
      "source": [
        "\n",
        "*   **Especificidad**: Fracción de verdaderos negativos. $\\frac{VN}{VN+FP}$\n",
        "*   **Sensibilidade/Recall**: Fraccióón de verdaderos positivos. $\\frac{VP}{VP+FN}$\n",
        "*   **F1 Score**: Promedio ponderado de precision y recall. Tiene en cuenta tanto los falsos positivos como los falsos negativos. (Aclarar como interpretarla)  $\\frac{2 \\times precision \\times recall}{precision + recall}$\n",
        "  \n",
        "\n",
        "*   **A área baixo a curva ROC (AUC - Area Under ROC)**: A curva ROC é a representación gráfica da sensibilidade frente á especificidade. AUC é a área baixo esa curva.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzwO6Bh3EOBi"
      },
      "source": [
        "## Graph of test and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1334bxrGzjJJ"
      },
      "source": [
        "def graph_train_val(model_name,train_loss,valid_loss, early, acc_train,acc_val, save):\n",
        "\n",
        "    plt.subplot(211)\n",
        "    plt.title(model_name)\n",
        "    plt.plot(range(0,len(train_loss)+1),[1]+train_loss, label='Training Loss')\n",
        "    plt.plot(range(0,len(valid_loss)+1),[1]+valid_loss,label='Validation Loss')\n",
        "    if(early):\n",
        "        plt.axvline(len(acc_val)-PATIENCE, 0, 1, label='Early stopping', color=\"r\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Train')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    \n",
        "    plt.subplot(212)\n",
        "    plt.title(model_name)\n",
        "    plt.plot(range(0,len(acc_train)+1),[0.5]+acc_train, label='Training acc')\n",
        "    plt.plot(range(0,len(acc_val)+1),[0.5]+acc_val,label='Validation acc')\n",
        "\n",
        "    if(early):\n",
        "        plt.axvline(len(acc_val)-PATIENCE, 0, 1, label='Early stopping', color=\"r\")\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracity')\n",
        "    plt.ylim(0.5, 1)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdU_13NXRKYE"
      },
      "source": [
        "def conf_matrix_plot(y_true, y_pred, title):\n",
        "  result = confusion_matrix(y_true, y_pred)\n",
        "  \n",
        "  ax = plt.axes()\n",
        "  sn.heatmap( result, annot=True, fmt=\"d\",ax=ax)\n",
        "  ax.set_xlabel(\"Predicted\")\n",
        "  ax.set_ylabel(\"Real\")\n",
        "  ax.set_title( title)\n",
        "  plt.show()\n",
        "  \n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "## Main\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_xti5eQ4ut"
      },
      "source": [
        "### Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvxNoLmQU1jI"
      },
      "source": [
        "def select_network(name, pretrained):\n",
        "    if 'Resnet50'==name: \n",
        "      return models.resnet50(pretrained=pretrained)\n",
        "    elif 'Resnet101'==name: \n",
        "      return models.resnet101(pretrained=pretrained)\n",
        "    elif 'VGG16'==name: \n",
        "      return models.vgg16(pretrained=pretrained)\n",
        "    elif 'VGG19'==name: \n",
        "      return models.vgg19(pretrained=pretrained)\n",
        "    elif 'Densenet161'==name:\n",
        "      return models.densenet161(pretrained=pretrained)\n",
        "    \n",
        "def select_optimizer(model, opt, lr): \n",
        "  if opt == 'Adam': #SGD: old version dont use6800\n",
        "      return optim.Adam(model.parameters(), lr=lr)\n",
        "  else:\n",
        "      raise \"NotImplementYet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6508fU9i2C0"
      },
      "source": [
        "def set_model(model, string, n_classes = 2): \n",
        "  \n",
        "  if ('VGG' == string[:3]):\n",
        "    model.classifier[-1] = nn.Linear(4096,n_classes, bias = True)   \n",
        "  elif ('Densenet' == string[:8]):\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, n_classes, bias = True)  \n",
        "  else:\n",
        "    model.fc = nn.Linear(model.fc.in_features, n_classes, bias = True) \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyHPvRaXRDo-"
      },
      "source": [
        "### Main a ejecutar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gE-Ez1qtyIA",
        "outputId": "650db4c2-4a2d-43e5-8fdd-9adfd734951e"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Chosen device :\", device)\n",
        "\n",
        "data_loaders = data_loader_fun(ROOT_DATASETS+DATASET)\n",
        "\n",
        "model = select_network(MODEL_STR, True)\n",
        "model = set_model(model, MODEL_STR)\n",
        "model.to(device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True  # Son entrenables = True\n",
        "    \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = select_optimizer(model, opt = OPTIM, lr = LR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chosen device : cuda:0\n",
            "Total images in dataset:  2623 \n",
            " Train: 1685  Val: 400  Test: 538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4mysV8tLQ_4",
        "outputId": "b7bd0fec-0c96-4d02-aece-d69a221f7efe"
      },
      "source": [
        "###### Train ######\n",
        "result_train = train_model(device = device, model = model, \n",
        "                           data_loaders = data_loaders, criterion = criterion, \n",
        "                           optimizer= optimizer, epochs = EPOCHS)\n",
        "\n",
        "train_loss, valid_loss, stop_epoch, acc_train,acc_val, early_stopping = result_train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "████░░░░░░░░░░░░░░░░ 21.5% "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi7yjBqAn_AD"
      },
      "source": [
        "early =  early_stopping.early_stop\n",
        "title = CLASS +\" \" + MODEL_STR + '- LR =' + str(LR)+' - '+OPTIM\n",
        "graph_train_val(title,train_loss,valid_loss, early, acc_train,acc_val, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14vuGnKELSrI"
      },
      "source": [
        "###### Test ######\n",
        "result_test = test(device, model, optimizer, data_loaders)\n",
        "y_pred, y_true, y_score, list_fp, list_fn  = result_test\n",
        "print(\"----- Probability of the labels\")\n",
        "print(\"Mean:\", np.around(np.mean(y_score), decimals = 3))\n",
        "print(\"Standard Deviation:\", np.around(np.std(y_score), decimals = 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNHnoOHLXkP"
      },
      "source": [
        "dic_metr = classification_report(y_true, y_pred, \n",
        "                                target_names= ['Neg', 'Pos'], output_dict=True)\n",
        "\n",
        "metricas = pd.DataFrame(dic_metr)\n",
        "\n",
        "auc = None\n",
        "if len(np.unique(y_true)) > 1:  # Si no hay neg da error,\n",
        "  auc = roc_auc_score(y_true, y_score)  \n",
        "\n",
        "result = conf_matrix_plot(y_true, y_pred,  title)\n",
        "\n",
        "tn, fp = result[0][0], result[0][1]\n",
        "fn, tp = result[1][0], result[1][1]\n",
        "\n",
        "tn, fp, fn, tp "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOfgo_v9nVQ4"
      },
      "source": [
        "\n",
        "print(\"Sensibilidad: \", str(tp/(tp+fn)))\n",
        "print(\"Especificidade: \", str(tn/(tn+fp))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EjJkBpi_sFD"
      },
      "source": [
        "### Save results, test and train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBAAZCMjBiHu"
      },
      "source": [
        "Dependencies: \n",
        "\n",
        "\n",
        "*   Train cells\n",
        "*   Test + Metric cells\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajENaDzzBCj2"
      },
      "source": [
        "root_dataset_info = ROOT_DATASETS+DATASET+'/info.csv'\n",
        "root = ROOT_DATA+'results/'+CLASS+'/'+DATASET+\"/\"+MODEL_STR+\"/results.csv\"\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv(root)\n",
        "  id = len(df)\n",
        "\n",
        "except FileNotFoundError:\n",
        "  df = pd.DataFrame([], columns = ['model', 'lr', 'optim','class', 'EarlyStopping',\n",
        "         'recall','specificity',  'auc', 'accuracy','fn','fp','tn','tp']) \n",
        "  id = 0\n",
        "\n",
        "model_root = ROOT_DATA+'results/'+CLASS+'/'+DATASET+\"/\"+MODEL_STR+\"/\"\n",
        "root_fp = model_root +\"fp/\"\n",
        "root_fn = model_root+\"fn/\"\n",
        "root_train = model_root+\"train/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYCaFJ6ik9qD"
      },
      "source": [
        "########## Checkpoint\n",
        "save = ROOT_DATA+'results/'+CLASS+'/'+DATASET+\"/\"+MODEL_STR+\"/\"\n",
        "try:\n",
        "  os.mkdir(save)\n",
        "except  FileExistsError:\n",
        "  print(\"ya existia esa carpeta:\", save)\n",
        "  \n",
        "try:\n",
        "  os.mkdir(root_fp)\n",
        "  os.mkdir(root_fn)\n",
        "  os.mkdir(root_train)\n",
        "\n",
        "except  FileExistsError:\n",
        "  \"It's okey\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J8CERKL7lxh"
      },
      "source": [
        "####### Warning: Create the net folder\n",
        "\n",
        "########## MODEL RESULTS RESULTS\n",
        "row = {'model': MODEL_STR, \n",
        "      'EarlyStopping': str(stop_epoch)+\"/\"+str(EPOCHS), \n",
        "      'accuracy': metricas['accuracy'][0],\n",
        "      'auc':  round(auc,3) if auc != None else np.nan, \n",
        "      'class': CLASS,\n",
        "      'specificity': metricas['Neg']['recall'],    #Binary case\n",
        "      'recall': metricas['Pos']['recall'],\n",
        "      'lr': LR, 'optim': OPTIM,\n",
        "       'batch_size': BATCH,\n",
        "       'fn':fn, 'fp':fp,\n",
        "       'tn':tn, 'tp':tp,\n",
        "       \"doubt_percentage\": MAX_DOUBT\n",
        "}    \n",
        "\n",
        "df = df.append(row, ignore_index=True)\n",
        "df.to_csv(root, index=False) \n",
        "\n",
        "########## TRAIN RESULTS\n",
        "df_train = pd.DataFrame([], columns = ['epoch','acc_train','acc_val','train_loss','val_loss'])\n",
        "df_train['acc_train'] = acc_train\n",
        "df_train['acc_val'] = acc_val\n",
        "df_train['train_loss'] = train_loss\n",
        "df_train['val_loss'] = valid_loss\n",
        "df_train['epoch'] = list(range(1,len(acc_train)+1))\n",
        "df_train.to_csv(root_train+str(id)+\".csv\", index=False) \n",
        "\n",
        "########### FP\n",
        "fp = pd.DataFrame([], columns = ['fp', 'dataset'])\n",
        "fp['dataset'] = np.repeat(root_dataset_info, len(list_fp))\n",
        "fp['fp'] =  list_fp\n",
        "fp.to_csv(root_fp+str(id)+\".csv\", index=False)\n",
        "\n",
        "########## FN\n",
        "fn = pd.DataFrame([], columns = ['fn', 'dataset'])\n",
        "fn['dataset'] = np.repeat(root_dataset_info, len(list_fn))\n",
        "fn['fn'] = list_fn\n",
        "fn.to_csv( root_fn+str(id)+\".csv\", index=False)\n",
        "\n",
        "print(id)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKwaNXg0ItFP"
      },
      "source": [
        "########## Checkpoint\n",
        "save = ROOT+'models/'+CLASS+'/'+DATASET+\"/\"+MODEL_STR+\"/\"\n",
        "try:\n",
        "  os.mkdir(save)\n",
        "except  FileExistsError:\n",
        "  print(\"ya existia esa carpeta:\", save)\n",
        "\n",
        "if early:\n",
        "  early_stopping.save_checkpoint(save+\"checkpoint.pt\")\n",
        "else:\n",
        "  torch.save(model.state_dict(),save+str(id)+\".pt\")\n",
        "\n",
        "  print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}